#!/usr/bin/env python3
#################################################################################
# Copyright 2018 ROBOTIS CO., LTD.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#################################################################################

# Authors: Gilbert #

import rospy
import os
import json
import numpy as np
import random
import time
import sys

from tqdm import tqdm
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque
from std_msgs.msg import Float32MultiArray
from src.turtlebot3_dqn.environment_stage_thesis import Env
from tensorflow.python.keras.models import Sequential, load_model
from tensorflow.keras.optimizers import RMSprop
from tensorflow.python.keras.layers import Dense, Dropout, Activation
import tensorflow as tf
EPISODES = 3000


class ReinforceAgent():
    def __init__(self, state_size, action_size):
        self.pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
        self.dirPath = os.path.dirname(os.path.realpath(__file__))
        self.dirPath = self.dirPath.replace('turtlebot3_dqn/nodes', 'turtlebot3_dqn/save_model/stage_4_')
        self.result = Float32MultiArray()

        self.load_model = True
        self.load_episode = 30
        self.state_size = state_size
        self.action_size = action_size
        self.episode_step = 6000
        self.target_update = 2000
        self.discount_factor = 0.99
        self.learning_rate = 0.00025
        self.epsilon = 1.0
        self.epsilon_decay = 0.99
        self.epsilon_min = 0.05
        self.batch_size = 64
        self.train_start = 64
        self.memory = deque(maxlen=1000000)

        self.model = self.buildModel()
        self.target_model = self.buildModel()

        self.updateTargetModel()

        if self.load_model:
            self.model.set_weights(load_model(self.dirPath + str(self.load_episode) + ".h5").get_weights())

            with open(self.dirPath + str(self.load_episode) + '.json') as outfile:
                param = json.load(outfile)
                self.epsilon = param.get('epsilon')

    def buildModel(self):
        model = Sequential()
        dropout = 0.2
        
        model.add(Dense(64, input_shape=(self.state_size,), activation='relu', kernel_initializer='lecun_uniform'))

        model.add(Dense(64, activation='relu', kernel_initializer='lecun_uniform'))
        model.add(Dropout(dropout))

        model.add(Dense(self.action_size, kernel_initializer='lecun_uniform'))
        model.add(Activation('linear'))
        model.compile(loss='mse', optimizer=RMSprop(lr=self.learning_rate, rho=0.9, epsilon=1e-06))
        model.summary()

        return model

    def getQvalue(self, reward, next_target, done):
        if done:
            return reward
        else:
            return reward + self.discount_factor * np.amax(next_target)

    def updateTargetModel(self):
        self.target_model.set_weights(self.model.get_weights())

    def getAction(self, state):
        if np.random.rand() <= self.epsilon:
            self.q_value = np.zeros(self.action_size)
            return random.randrange(self.action_size)
        else:
            q_value = self.model.predict(state.reshape(1, len(state)))
            self.q_value = q_value
            return np.argmax(q_value[0])

    def appendMemory(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def trainModel(self, target=False):
        mini_batch = random.sample(self.memory, self.batch_size) #pseudo-random number generators 
        X_batch = np.empty((0, self.state_size), dtype=np.float64) #state vector shape (0,state_size)
        Y_batch = np.empty((0, self.action_size), dtype=np.float64) #action vector shape (0,action_size)

        with tf.device('/GPU:0'):
            for i in range(self.batch_size):
                states = mini_batch[i][0]
                actions = mini_batch[i][1]
                rewards = mini_batch[i][2]
                next_states = mini_batch[i][3]
                dones = mini_batch[i][4]

                q_value = self.model.predict(states.reshape(1, len(states)))
                #pytorch Q.foward
                self.q_value = q_value

                if target:
                    next_target = self.target_model.predict(next_states.reshape(1, len(next_states)))

                else:
                    next_target = self.model.predict(next_states.reshape(1, len(next_states)))

                next_q_value = self.getQvalue(rewards, next_target, dones)

                X_batch = np.append(X_batch, np.array([states.copy()]), axis=0)
                Y_sample = q_value.copy()

                Y_sample[0][actions] = next_q_value
                Y_batch = np.append(Y_batch, np.array([Y_sample[0]]), axis=0)

                if dones:
                    X_batch = np.append(X_batch, np.array([next_states.copy()]), axis=0)
                    Y_batch = np.append(Y_batch, np.array([[rewards] * self.action_size]), axis=0)
        with tf.device('/GPU:1'):
            self.model.fit(X_batch, Y_batch, batch_size=self.batch_size, epochs=1, verbose=0)
        print ("------Finish training---------")

if __name__ == '__main__':
    rospy.init_node('turtlebot3_dqn_stage_thesis')
    pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
    pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)
    result = Float32MultiArray()
    get_action = Float32MultiArray()

    state_size = 28
    action_size = 5
    rospy.logdebug("------Init ENV")
    env = Env(action_size)

    agent = ReinforceAgent(state_size, action_size)
    scores, episodes = [], []
    global_step = 0
    start_time = time.time()

    for e in tqdm(range(agent.load_episode + 1, EPISODES)):
        done = False
        state = env.reset()
        score = 0

        for t in range(agent.episode_step):

            action = agent.getAction(state)
            next_state, reward, done = env.step(action)
            agent.appendMemory(state, action, reward, next_state, done)
            #learn from sars'tupe
            if len(agent.memory) >= agent.train_start:
                print ("------Start training---------")
                env.pause()
                if global_step <= agent.target_update: 
                    agent.trainModel()
                else:
                    agent.trainModel(True) #using target model to predict actions
           
            score += reward
            env.unpause()
            state = next_state
            get_action.data = [action, score, reward]
            pub_get_action.publish(get_action)

            if e % 10 == 0: #save model after each 10 ep.
                agent.maodel.save(agent.dirPath + str(e) + '.h5')
                with open(agent.dirPath + str(e) + '.json', 'w') as outfile:
                    json.dump(param_dictionary, outfile)

            if t >= 500: #take so long to reach goal
                rospy.loginfo("Time out!!")
                done = True

            if done:
                result.data = [score, np.max(agent.q_value)]
                pub_result.publish(result)
                agent.updateTargetModel()
                scores.append(score)
                episodes.append(e)
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)

                rospy.loginfo('Ep: %d score: %.2f memory: %d epsilon: %.2f time: %d:%02d:%02d',
                              e, score, len(agent.memory), agent.epsilon, h, m, s)
                param_keys = ['epsilon']
                param_values = [agent.epsilon]
                param_dictionary = dict(zip(param_keys, param_values))
                break

            global_step += 1
            if global_step % agent.target_update == 0:
                rospy.loginfo("UPDATE TARGET NETWORK")

        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay




''' import rospy
import os
import json
import numpy as np
import time
import sys


from tqdm import tqdm
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from std_msgs.msg import Float32MultiArray
from src.turtlebot3_dqn.environment_stage_thesis import Env

EPISODES = 3000

import torch as T


import numpy as np

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class ReplayBuffer():
    def __init__(self, max_size, input_dims, batch_size):
        self.mem_size = max_size
        self.mem_cntr = 0 #keep track position of position of first unsave mem
        self.batch_size = batch_size
        #mem for states unpack input dims as a list *
        self.state_memory  = np.zeros((self.mem_size, *input_dims),
                                      dtype = np.float32)
        #mem for state trasition
        self.new_state_memory = np.zeros ((self.mem_size, *input_dims),
                                          dtype =np.float32)
        #mem for action   
        self.action_memory = np.zeros (self.mem_size, dtype =np.int32)
        #reward memory
        
        self.reward_memory = np.zeros (self.mem_size, dtype =np.float32)
        #mem for terminal
        self.terminal_memory = np.zeros (self.mem_size, dtype =np.bool)
    
    #add the trasition to the memory buffer
    def store_transition (self, state, action, reward, new_state , done):
        
        #check the unocupied mem position
        index = self.mem_cntr % self.mem_size
        
        #update the replay buffer
        self.state_memory[index]= state
        self.new_state_memory[index] = new_state
        self.action_memory[index] = action
        self.terminal_memory[index] = 1 - int (done) #want to multiply the reward by 0 (can also do it in learn function)
        self.mem_cntr+=1 # increase the memory
        
    #function to sample from the memmory from the batch size    
    def sample_buffer (self, batch_size):
        #have we fill up the mem or not? 
        max_mem = min (self.mem_cntr, self.mem_size)
        
        #select randomly from batch
        batch = np.random.choice(max_mem, batch_size, replace = False)# wont select again with replay False
        # book keeping for proper batch slicing
        batch_index = np.arange (self.batch_size, dtype=np.int32)
        
        
        #return the data in batch
        states = self.state_memory[batch]
        states_=  self.new_state_memory[batch]
        rewards =  self.reward_memory[batch]
        actions =  self.action_memory[batch] 
        terminal = self.terminal_memory[batch]
        
        return states, actions, rewards, states_, terminal, batch_index

class LinearDeepQNetwork(nn.Module):
    
    #constructor 
    def __init__(self, lr, n_actions, input_dims):
        # run the constructor of the parent class.
        super(LinearDeepQNetwork,self).__init__()
        
        # 3 steps:
        # 1: define the layers
        # 2: define the optimizer, and lose funtion: torch.optim, torch.nn
        # 3: define the training devices and send it to the device.: torch.device
        
        self.fc1 = nn.Linear(*input_dims, 256)
        self.fc2 = nn.Linear (256, n_actions)
        #choose gradient decent method for backward propergation
        self.optimizer = optim.Adam(self.parameters(),lr = lr)
        self.loss = nn.MSELoss()
        
        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')
        #send the network to the device
        self.to(self.device)
        
        
    #forward propergation: activation function
    def forward(self, state):
        
        layer1 = F.relu(self.fc1(state))
        out_actions = self.fc2(layer1)
        
        return out_actions

class ReinforceAgent():
    def __init__(self, state_size, action_size):
        self.pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
        self.dirPath = os.path.dirname(os.path.realpath(__file__))
        self.dirPath = self.dirPath.replace('turtlebot3_dqn/nodes', 'turtlebot3_dqn/save_model/stage_4_')
        self.result = Float32MultiArray()

        self.load_model = True
        self.load_episode = 30
        self.state_size = state_size
        self.observation_space = (self.state_size,)

        self.action_size = action_size
        self.action_space = np.arange(0,5,1) # for similar to gym
        
        
        
        self.episode_step = 6000
        self.target_update = 2000
        self.discount_factor = 0.99
        self.learning_rate = 0.00025
        self.epsilon = 1.0
        self.epsilon_decay = 0.99
        self.epsilon_min = 0.05
        self.batch_size = 64
        self.train_start = 64
        
        self.global_step = 0
        
        self.mem_size = 1000000
        #self.memory = deque(maxlen=1000000)
        self.memory = ReplayBuffer(self.mem_size, self.observation_space,self.batch_size)
        
        self.model = LinearDeepQNetwork(self.learning_rate,  self.action_size, self.observation_space)
        self.target_model = LinearDeepQNetwork(self.learning_rate,  self.action_size, self.observation_space)
        #self.model = self.buildModel()
        #self.target_model = self.buildModel()
        
        self.updateTargetModel()

        # if self.load_model:
        #     self.model.set_weights(load_model(self.dirPath + str(self.load_episode) + ".h5").get_weights())

        #     with open(self.dirPath + str(self.load_episode) + '.json') as outfile:
        #         param = json.load(outfile)
        #         self.epsilon = param.get('epsilon')
    #set weights of neurons to target model
    def updateTargetModel(self): 
        self.target_model.load_state_dict(self.model.state_dict())
    
    def choose_action (self, observation):
        if np.random.random() < self.epsilon:
            action = np.random.choice(self.action_space)
        else:
            state = T.tensor(observation, dtype= T.float).to(self.model.device)
            actions = self.model.forward(state)
            na = actions.to('cpu').detach().numpy() # we have T.argmax () dont have to transfer to np by item ()
            action = np.argmax(na)
        return action

    
    #using ReplayBuffer    
    def store_trasition(self, state, action, reward, new_state, done):
        self.memory.store_transition(state, action, reward, new_state, done)
    #def appendMemory(self, state, action, reward, next_state, done):
    #   self.memory.append((state, action, reward, next_state, done))
    print ("------Not training---------")
    def learn(self):
        if (self.memory.mem_cntr < self.train_start):
            self.q_value = 0
            return

        print ("------Start training---------")
        #env.pause()
        if self.global_step <= agent.target_update: 
            target = False
        else:
            target = True #using target model to predict actions
       
       
        states, actions, rewards, states_ ,terminal, batch_index= self.memory.sample_buffer(self.memory.batch_size)
        
        states = T.tensor(states, dtype= T.float).to(self.model.device)
        #actions = T.tensor(actions).to(self.model.device) # dont need to be a tensor
        rewards = T.tensor(rewards).to(self.model.device)
        states_ = T.tensor(states_, dtype= T.float).to(self.model.device)
        terminal = T.tensor(terminal).to(self.model.device)
        
         
        q_prediction = self.model.forward(states)[batch_index, actions]
        self.q_value =  q_prediction  
        
        if target:
            q_next = self.target_model.forward(states_).max()
        else:
            q_next = self.model.forward(states_).max()
            
        
        #q_next [terminal] = 0 ##explain
        q_target = rewards + self.discount_factor*q_next
        
        loss = self.model.loss(q_target, q_prediction).to(self.model.device) # is the TD error
        loss.backward()
        self.model.optimizer.step()
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        else:
            self.epsilon =  self.epsilon_min
        
        print ("------Finish training---------")

if __name__ == '__main__':
    rospy.init_node('turtlebot3_dqn_stage_thesis')
    pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
    pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)
    result = Float32MultiArray()
    get_action = Float32MultiArray()

    state_size = 28
    action_size = 5
    rospy.logdebug("------Init ENV")
    env = Env(action_size)

    agent = ReinforceAgent(state_size, action_size)
    scores, episodes = [], []
    agent.global_step = 0
    start_time = time.time()

    for e in tqdm(range(agent.load_episode + 1, EPISODES)):
        done = False
        state = env.reset()
        score = 0

        for t in range(agent.episode_step):

            action = agent.choose_action(state)
            next_state, reward, done = env.step(action)
            agent.store_trasition(state, action, reward, next_state, done)
            #learn from sars'tupe
            score += reward
            agent.learn()
            state = next_state
            
            get_action.data = [action, score, reward]
            pub_get_action.publish(get_action)

            # if e % 10 == 0: #save model after each 10 ep.
            #     agent.model.save(agent.dirPath + str(e) + '.h5')
            #     with open(agent.dirPath + str(e) + '.json', 'w') as outfile:
            #         json.dump(param_dictionary, outfile)

            if t >= 500: #take so long to reach goal
                rospy.loginfo("Time out!!")
                done = True

            if done:
                #result.data = [score, np.max(agent.q_value) ]# explain
                pub_result.publish(result)
                agent.updateTargetModel()
                scores.append(score)
                episodes.append(e)
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)

                rospy.loginfo('Ep: %d score: %.2f epsilon: %.2f time: %d:%02d:%02d',
                              e, score, agent.epsilon, h, m, s)
                param_keys = ['epsilon']
                param_values = [agent.epsilon]
                param_dictionary = dict(zip(param_keys, param_values))
                break

            agent.global_step += 1
            if agent.global_step % agent.target_update == 0:
                rospy.loginfo("UPDATE TARGET NETWORK")
                agent.updateTargetModel() '''

        